\begin{frame}{Question E3.1}
	\QuestionKCs{continuity}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{Consider the function
	\begin{center}
		\includegraphics[width=0.25\textwidth]{absx.png}
    \end{center}
    Which one of the following is true?}
	\QuestionAnswers
	{
		\correctanswer it is $C^0$
		\answer it is $C^1$
		\answer it is $C^2$		
		\answer it is all of them
		\answer I do not know
	}
	\note<1->{\begin{itemize}
		\item 
	\end{itemize}}
\end{frame}

\begin{frame}{Question E3.2}
	\QuestionKCs{logarithms}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{The equivalence $x y^m = e^n \sin(y)$ can be transformed into}
	\QuestionAnswers
	{
		\answer $\log_e(x)+m \log_e(y)=n+ \log_e(\sin(y))$
		\answer $m \log_e(xy)=n+\log_e(\sin(y))$
		\answer $\log_e(x)+m\log_e(y)=n \log_e(e) + \log_e(\sin(y))$
		\answer $\log_e(x)+mlog_e(y) = n \log_e(e) \log_e(\sin(y))$		
		\correctanswer 1 and 3 
		\answer 2 and 3
		\answer I do not know
	}
\end{frame}

\begin{frame}{Question E3.3}
	\QuestionKCs{maximization}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{$\arg \max_{x \in \Reals} \left( -(x-5)^2+7 \right)=$}
	\QuestionAnswers
	{
		\answer $1$
		\correctanswer $5$
		\answer $7$
		\answer $5$ or $7$		
		\answer I do not know 
	}
\end{frame}

\begin{frame}{Question E3.4}
	\QuestionKCs{parametric methods,nonparametric methods}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{If we fix a finite-dimensional hypothesis space before seeing the data then we are using a \ldots}
	\QuestionAnswers
	{
		\correctanswer parametric method
		\answer nonparametric method
		\answer I do not know
	}
\end{frame}

\begin{frame}{Question E3.6}
	\QuestionKCs{statistic}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody
	{
		Consider
		$$
			\bm{y} = y_{1}, \ldots, y_{n},
			\qquad
			y_{t} \sim \mathcal{N} \left( m, \sigma^{2} \right) \; \text{i.i.d.},
			\qquad
			\phi \left( \bm{y} \right) = m \left\| \bm{y} \right\|^{2}.
		$$
		Is $\phi$ a stastistic of y?}
	\QuestionAnswers
	{
		\answer yes
		\correctanswer no
		\answer I do not know
	}
\end{frame}


\begin{frame}{Question E3.8}
	\QuestionKCs{statistic}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody{typically, a statistic is a random variable}
	\QuestionAnswers
	{
		\correctanswer true
		\answer false
		\answer I do not know
	}
\end{frame}


\begin{frame}{Question E3.10}
	\QuestionKCs{statistic}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody{the main difference between a statistic and a parameter (or set of parameters) is that \ldots}
	\QuestionAnswers
	{
		\answer the statistic always lives in a different domain
		\answer the parameter is multidimensional
		\correctanswer the statistic is observable and the parameter not
		\answer I don't know
	}
\end{frame}


\begin{frame}{Question E3.12}
	\QuestionKCs{estimator}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{an estimator is a function that always returns one single point, never sets of points}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I do not know
	}
\end{frame}

\begin{frame}{Question E3.14}
	\QuestionKCs{estimator}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{an estimator is always a random variable}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I do not know
	}
\end{frame}


\begin{frame}{Question E3.16}
	\QuestionKCs{function analysis}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody{if a function is bounded over a compact then it admits maximum and minimum}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I do not know
	}
\end{frame}

\begin{frame}{Question E3.19}
	\QuestionKCs{estimator}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		Consider the problem
		$$
			\thetahat = \arg \min_{\theta} \left\| \bm{y} - \bm{f} \left( \bm{u}, \theta \right) \right\|^{2} .
		$$
		This estimation problem is well written
	}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I do not know
	}
\end{frame}


\begin{frame}{Question E3.21}
	\QuestionKCs{separability}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{How many solutions may a separable LS problem have?}
	\QuestionAnswers
	{
		\answer either 0 or 1
		\answer either 0, 1, or $N$ with $N$ countable
		\correctanswer either 0, 1, or $N$ with $N$ uncountable
		\answer I don't know
	}
\end{frame}


\begin{frame}{Question E3.23}
	\QuestionKCs{separability}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{separable LS problems always admit closed form solutions}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I do not know
	}
\end{frame}


\begin{frame}{Question E3.24}
	\QuestionKCs{separability,systems of linear equations}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody{in a separable LS problem of the type $\bm{y} = \bm{\phi} \theta + \bm{v}$, assume that $\bm{\phi}$ is square and non-singular. Then the number of solutions is \ldots}
	\QuestionAnswers
	{
		\correctanswer exactly one
		\answer at most one
		\answer at most infinity
		\answer I don't know
	}
\end{frame}


\begin{frame}{Question E3.26}
	\QuestionKCs{least squares}
	\QuestionKCsTaxonomies{(2,1)}
	\QuestionNotes{}
	\QuestionBody
	{
		Consider the normal equations associated to
    	$$
			\bm{y} = \Phi(\bm{u}) \bm{\theta} + \bm{e} .
    	$$
		How many solutions will the least squares problem admit if $\Phi(\bm{u})^{T} \bm{y}$ is in the column space of $\Phi(\bm{u})^{T} \Phi(\bm{u})$?
	}
	\QuestionAnswers
	{
		\answer 0
		\correctanswer 1
		\answer $+\infty$
		\answer I do not know
	}
	\note<1->{\begin{itemize}
		\item
	\end{itemize}}
\end{frame}


\begin{frame}{Question E3.28}
	\QuestionKCs{maximum likelihood}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		$$
			p(y ; \theta) \in C^{0} \left( \Theta \right) \implies \exists \thetaML
		$$
	}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I don't know
	}
	\note<1->{\begin{itemize}
		\item $\Theta$ may be open, and happen the same thing that happened with the existence of the least squares estimator
	\end{itemize}}
\end{frame}

\begin{frame}{Question E3.30}
	\QuestionKCs{maximum likelihood}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		$$
			\left\lbrace
			\begin{array}{l}
				p(y ; \theta) \in C^{0} \left( \Theta \right) \\
				\Theta \text{ compact}
			\end{array}
			\right.
			\implies \exists \thetaML
		$$
	}
	\QuestionAnswers
	{
		\correctanswer true
		\answer false
		\answer I don't know
	}
	\note<1->{\begin{itemize}
		\item now here we can apply Weierstrass theorem
	\end{itemize}}
\end{frame}


\begin{frame}{Question E3.32}
	\QuestionKCs{maximum likelihood}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		$$
			\exists \thetaML
			\text{ only if }
			\left\lbrace
			\begin{array}{l}
				p(y ; \theta) \in C^{0} \left( \Theta \right) \\
				\Theta \text{ compact}
			\end{array}
			\right.
		$$
	}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I don't know
	}
	\note<1->{\begin{itemize}
		\item the previous conditions are sufficient, not necessary
	\end{itemize}}
\end{frame}


\begin{frame}{Question E3.34}
	\QuestionKCs{maximum likelihood}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		$$
			\left\lbrace
			\begin{array}{l}
				p(y ; \theta) \in C^{0} \left( \Theta \right) \\
				\Theta \text{ compact}
			\end{array}
			\right.
			\implies \exists! \thetaML
		$$
	}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I don't know
	}
	\note<1->{\begin{itemize}
		\item like in the least squares case, it is not guaranteed that the existence is unique
	\end{itemize}}
\end{frame}


\begin{frame}{Question E3.36}
	\QuestionKCs{maximum likelihood}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		$$
			\nabla \ell \left( \thetaML \right) = \bm{0}
		$$
	}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I don't know
	}
	\note<1->{\begin{itemize}
		\item the ML estimate may be on the border of the domain
	\end{itemize}}
\end{frame}


\begin{frame}{Question E3.38}
	\QuestionKCs{maximum likelihood}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		$$
			\thetaML \in \text{int }\Theta
			\quad
			\implies
			\quad
			\nabla \ell \left( \thetaML \right) = \bm{0}
		$$
	}
	\QuestionAnswers
	{
		\answer true
		\correctanswer false
		\answer I don't know
	}
	\note<1->{\begin{itemize}
		\item the gradient may not even be defined, even if we have a continuous $\ell$. Imagine something shaped like a \^, for example
	\end{itemize}}
\end{frame}


\begin{frame}{Question E3.40}
	\QuestionKCs{maximum likelihood}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		$$
			\left\lbrace
			\begin{array}{l}
				\thetaML \in \text{int }\Theta \\
				\nabla \ell \exists \text{ in int} \Theta
			\end{array}
			\right.
			\quad
			\implies
			\quad
			\nabla \ell \left( \thetaML \right) = \bm{0}
		$$
	}
	\QuestionAnswers
	{
		\correctanswer true
		\answer false
		\answer I don't know
	}
	\note<1->{\begin{itemize}
		\item now the sufficient conditions are satisfied
	\end{itemize}}
\end{frame}


\begin{frame}{Question E3.42}
	\QuestionKCs{independence,maximum likelihood}
	\QuestionKCsTaxonomies{(1,1)}
	\QuestionNotes{}
	\QuestionBody
	{
		Which assumption are we implicitly using when we write a ML estimation problem as
		$$
			\arg \min_{\theta \in \Theta}
			\sum_{t} \log \ell \left( y_{t} ; \theta \right)
			\; ?
		$$
	}
	\QuestionAnswers
	{
		\answer the distribution of the $y_{t}$'s is separable
		\answer the $y_{t}$'s are iid
		\correctanswer the $y_{t}$'s are independent
		\answer I don't know
	}
\end{frame}




\begin{frame}{Question E3.44}
	\QuestionKCs{probability,likelihood}
	\QuestionKCsTaxonomies{(1,2),(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		Probability refers to events of the \ldots\ldots, while likelihood refers to events of the \ldots\ldots (fill the blanks)
	}
	\QuestionAnswers
	{
		\answer past, past
		\answer past, future
		\correctanswer future, past
		\answer future, future
		\answer I don't know
	}
\end{frame}


\begin{frame}{Question E3.46}
	\QuestionKCs{maximum likelihood,least squares}
	\QuestionKCsTaxonomies{(1,2),(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		Under which assumptions ML and LS become the same?
	}
	\QuestionAnswers
	{
		\answer linear model and additive Gaussian noises
		\answer linear model
		\correctanswer additive Gaussian noise
		\answer I don't know
	}
\end{frame}


\begin{frame}{Question E3.48}
	\QuestionKCs{optimization}
	\QuestionKCsTaxonomies{(1,2)}
	\QuestionNotes{}
	\QuestionBody
	{
		When doing $\arg \max$, instead of using the transformation $- \log( \cdot )$, would it have been formally correct to use the transformation $\sin ( \cdot )$?
	}
	\QuestionAnswers
	{
		\answer yes
		\answer no
		\correctanswer depends on the domain
		\answer I don't know
	}
\end{frame}



